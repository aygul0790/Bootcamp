{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aygul0790/Bootcamp/blob/main/Day_1/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3O7DtoekbvX"
      },
      "source": [
        "# Transformers\n",
        "\n",
        "### What we will cover today:\n",
        "\n",
        "- Why should you care about Transformers?\n",
        "\n",
        "- RNNs: Problems and progress\n",
        "\n",
        "- Transformers: Context-aware embeddings\n",
        "\n",
        "- Digging Deeper: What we missed\n",
        "\n",
        "- The transformers Library\n",
        "\n",
        "- Going further: GPT, BERT & other models\n",
        "\n",
        "- Recap & Further reading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnXOv2wukbva"
      },
      "source": [
        "## 1Ô∏è‚É£ Why should you care about Transformers?\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/chatgpt.jpg?raw=1\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RpiriT7kbva"
      },
      "source": [
        "## 2Ô∏è‚É£ RNNs: Problems and progress\n",
        "\n",
        "Before we dive into Transformers, let's remind ourselves how a lot of NLP tasks are often tackled by RNNs:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/rnn_problem.png?raw=1\" width=\"750\"/>\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/encoder_decoder.png?raw=1\" width=\"750\"/>\n",
        "\n",
        "\n",
        "### What are the key issues we face here?\n",
        "\n",
        "- Information bottleneck at interface\n",
        "- Vanishing gradient problem\n",
        "- We have to compute the entire sequence recursively (makes scaling very hard!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7q0c3X-kbvb"
      },
      "source": [
        "### RNNs suffer from vanishing gradient through time\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/gradient_vanishing.png?raw=1\" width=\"750\"/>\n",
        "\n",
        "‚ùóÔ∏èBackpropagation through time‚ùóÔ∏è Within a single layer RNN model.\n",
        "\n",
        "- During backpropagation, the gradient vanishes to 0 as the time step decreases.\n",
        "- As a result, simple RNNs are said to have short memory (even with variants like LSTM and GRU)\n",
        "\n",
        "üìö Michael Phi - [Illustrated Guide to Recurrent Neural Networks](https://medium.com/data-science/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
        "\n",
        "### What does this mean for our performance?\n",
        "\n",
        "A simplification of problems with RNNs:\n",
        "\n",
        "- Sally adored reading; when she received a book on her birthday she was older\n",
        "\n",
        "What we'd like:\n",
        "\n",
        "- Sally adored reading; when she received a book on her birthday she was happy!\n",
        "\n",
        "RNNs are likely to miss out on **important context** from earlier in the sentence because of their recency bias ü´†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK_fcdZwkbvb"
      },
      "source": [
        "## 3Ô∏è‚É£ Transformers\n",
        "\n",
        "### The paper that started it all: [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/attention_model.png?raw=1\" width=\"400\"/>\n",
        "\n",
        "The highest level view:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/high_level.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "Broken down a bit more:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/encoder_decoder_simplified.png?raw=1\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76W1xncokbvc"
      },
      "source": [
        "### Before we talk about what's going on inside the encoder layers, let's talk about what's going into it!\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/numbered_embedding_diagram.png?raw=1\" width=\"400\"/>\n",
        "\n",
        "\n",
        "Zooming in on one of the encoders:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/768_transformer_block.png?raw=1\" width=\"400\"/>\n",
        "\n",
        "\n",
        "- Usually the **embedding size** is 768\n",
        "- The **hidden dimension** (the length of the projected Q, K and V vectors) is also 768\n",
        "- In the example we'll use size 2 for simplicity\n",
        "\n",
        "‚ùì What's going on in this strange self-attention layer?\n",
        "\n",
        "1) Each token (word) embedding gets **projected** ‚û°Ô∏è into 3 further vectors: the **query, key and value** vectors\n",
        "\n",
        "2) We compute a **scaled dot-product** üî¥ on the query and key vectors to work out how much each word relates to those around it\n",
        "\n",
        "3) Take these scores and **normalize with softmax** ‚§µÔ∏è\n",
        "\n",
        "4) **Multiply by our value vectors** ‚ùé, sum and pass to our dense neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL_FXMBgkbvc"
      },
      "source": [
        "#### An example sentence\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/sentence.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/qkv_cleaned.png?raw=1\" width=\"400\"/>\n",
        "\n",
        "Each of these three vectors are learned as the model sees more data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWTff9Gbkbvc"
      },
      "source": [
        "#### Three zoomed-in examples:\n",
        "\n",
        "Two words in a sentence that are closely related\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/dot_product_related_v2.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "The same two words but seen from the other perspective:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/dot_product_related_inverse_v2_1.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "Finally, two words with a weak connection:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/dot_product_unrelated_v2_1.png?raw=1\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeTXsXomkbvd"
      },
      "source": [
        "#### Let's look at one dot product\n",
        "\n",
        "To keep it really simple, we're going to imagine our Q, K and V have only been projected into two dimensions\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/01_one_Q_one_K_v2.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "#### What happens once we have our dot products?\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/02_one_Q_all_K_v2.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "#### Then we scale:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/03_one_Q_all_K_scaled_v2.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "#### Finally we apply softmax:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/04_one_Q_all_K_softmaxed_v2.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "We have to do this for each word in our sentence!\n",
        "\n",
        "You can see how this becomes a matrix operation\n",
        "\n",
        "We get the scaled dot-product attention for all of our Query and Key vectors:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/05_attention_matrix_v2.png?raw=1\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjDMywUSkbvd"
      },
      "source": [
        "#### And then multiply our \"similarity score\" with all of our Value vectors\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/06_one_attention_all_V_v2_1.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/07_all_attention_all_V_v2_1.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "So really the entire thing can be written like this:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/QKV_matrix_method.png?raw=1\" width=\"600\"/>\n",
        "\n",
        "\n",
        "We are done with our multiplications!\n",
        "\n",
        "Now we just need to normalize and pass through to the feed-forward neural network\n",
        "\n",
        "The neural network will output vectors of our original embedding dimension (e.g. 768)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggbVGIUskbvd"
      },
      "source": [
        "### Putting it all together:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/slowest_qkv.gif?raw=1\" width=\"950\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnxYvqmbkbvd"
      },
      "source": [
        "### Let's check the lecture notebook for some Tensorflow visualizations\n",
        "\n",
        "Computing one set of all of these Q, K, V multiplications and processes is what we call \"single-headed attention\".\n",
        "\n",
        "When we are doing our initial linear projections (used to create the Q, K and V vectors) we can express these operations as matrices of weights too!\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/QKV_matrix.png?raw=1\" width=\"400\"/>\n",
        "\n",
        "With that in mind, we see the complexity of our model compared to the others:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/complexity.png?raw=1\" width=\"400\"/>\n",
        "\n",
        "Context windows (a.k.a. our max sequence lengths) add a lot of weights, however they have been [getting much larger lately](https://www.anthropic.com/news/100k-context-windows)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO8yI_Nmkbve"
      },
      "source": [
        "### Multi-headed?\n",
        "\n",
        "- We can use multiple heads to split up and analyze different parts of our embedding\n",
        "- Each can focus a different part of the embedding eg. working on semantic vs. syntactic features of our sentences\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/multi_headed.png?raw=1\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAkPVbnkkbvf"
      },
      "source": [
        "### Let's check in with our original diagram:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/one_encoder_block.png?raw=1\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW95cGrBkbvf"
      },
      "source": [
        "#### What about the decoder?\n",
        "\n",
        "At the highest level view, its job is to choose the **most likely next token**:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/decoder_high_veiw.png?raw=1\" width=\"600\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AscNvrxVkbvg"
      },
      "source": [
        "### How does it do this? And what happened to all the work the encoder did?\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/decoder_training_step_1.png?raw=1\" width=\"700\"/>\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/decoder_training_step_2.png?raw=1\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpTmNB2qkbvg"
      },
      "source": [
        "### Cross-Attention\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/cross-attention-in-transformer-decoder.png?raw=1\" width=\"700\"/>\n",
        "\n",
        "\n",
        "Attention between encoder and decoder (a.k.a. cross-attention):\n",
        "\n",
        "- **Self-attention** operates within a single sequence and captures the relationships between tokens within that sequence.\n",
        "- **Cross-attention** operates between two different sequences and captures the relationships between tokens from the source sequence and tokens from the target sequence, allowing the model to generate relevant output based on the information in the source sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjnDJEb6kbvg"
      },
      "source": [
        "## Let's recap the training process at a high level\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/decoder_training_highest.png?raw=1\" width=\"700\"/>\n",
        "\n",
        "### So what does inference look like?\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/inference.png?raw=1\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD7ErPyCkbvg"
      },
      "source": [
        "### That is all here!\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/attention_model.png?raw=1\" width=\"400\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai7SwNMQkbvg"
      },
      "source": [
        "## Digging Deeper\n",
        "\n",
        "#### What haven't we covered yet:\n",
        "- Self-padding mask\n",
        "- Skip layers\n",
        "- Subword tokenization\n",
        "- Positional encoding\n",
        "\n",
        "#### Self-padding mask\n",
        "\n",
        "- Masks out padding tokens during self-attention, preventing the model from attending to them and ensuring focus only on relevant parts of variable-length input sequences.\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/padding_in_practice_encoder.png?raw=1\" width=\"700\"/>\n",
        "\n",
        "#### Skip Layers\n",
        "\n",
        "- Information from earlier layers is propagated directly to later layers, aiding in the retention of valuable information during training.\n",
        "- Faster convergence and improved performance ‚úÖ\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/skip_layers.png?raw=1\" width=\"500\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeX4TPyfkbvg"
      },
      "source": [
        "### Subword tokenization\n",
        "- Enables the model to handle OOV words and capture morphological variations (e.g. \"walks\" and \"walking\" aren't viewed as entirely separate tokens but rather composites of shared/ different tokens)\n",
        "- Makes the language representation more compact, efficient, and generalizable across different word forms\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/subword.svg?raw=1\" width=\"700\"/>\n",
        "\n",
        "### Positional encoding\n",
        "- Gives the model an idea of each word's position in the sentence\n",
        "- Can be hard-coded or learned\n",
        "- Attention is All You Need has a clever hard-coding using varying frequencies to convey positional information\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/numbered_embedding_diagram.png?raw=1\" width=\"400\"/>\n",
        "\n",
        "See [this video](https://www.youtube.com/watch?v=dichIcUZfOw) for a detailed walkthrough!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu-qervZkbvg"
      },
      "source": [
        "## 5Ô∏è‚É£ HuggingFace\n",
        "Pretty tricky under the hood but the transformers makes it all super easy in code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4vWFGfuSkbvg"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVqjReB1kbvh"
      },
      "source": [
        "- Pre-trained Models: Provides a collection of pre-trained state-of-the-art models like BERT, GPT-2, T5, and many more for various NLP tasks.\n",
        "- Simple: Offers a unified and simple way to find a model, fine-tune it, and deploy it.\n",
        "- Multilingual: Supports models in multiple languages (although lots are primarily written in torch but there are plenty of tricks that enable you to work with PyTorch models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OX2EnTukbvh"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BitivoDBkbvh"
      },
      "outputs": [],
      "source": [
        "result = pipe(\"I am a student and I am studying in London\")\n",
        "result[0]['translation_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DycnrVTkbvh"
      },
      "source": [
        "### Under the hood of a pipeline\n",
        "- Models use their own trained **tokenizers** which we load up with the `.from_pretrained()` method\n",
        "- Then we just pass in the creator of the model and model name\n",
        "- If we want to pass in Tensorflow tensors (which we will), just put TF before our model and pass `from_pt = True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai-q_Us3kbvh"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "\n",
        "tokens = tokenizer.encode(\"This is easy!\", return_tensors = \"tf\")\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7usqpu0kbvh"
      },
      "outputs": [],
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\", from_pt = True)\n",
        "\n",
        "output_tokens = model.generate(tokens)\n",
        "\n",
        "print(output_tokens)\n",
        "\n",
        "print(tokenizer.decode(output_tokens[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCKFeBcCkbvi"
      },
      "source": [
        "## 6Ô∏è‚É£ Going further\n",
        "\n",
        "### What are the two parts of our diagram doing?\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/decoder_training_highest.png?raw=1\" width=\"700\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSSuF_17kbvi"
      },
      "source": [
        "### The Transformer family tree:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/TF_fam_tree.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "- Encoder-only (e.g. BERT): converts an input sequence to a numerical representations. Uses words to the left and right of each word (hence \"bidirectional\") and is great for things like classification.\n",
        "\n",
        "- Decoder-only (e.g. GPT): takes an input sequence and iteratively predicts the most likely next word (can also be used in a similar manner to encoder-decoder if trained correctly)\n",
        "\n",
        "- Encoder-decoder (e.g. T5 or original \"Attention is all you need\" paper model): maps one sequence to another"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAIqC4hJkbvi"
      },
      "source": [
        "### So what makes ChatGPT so great?\n",
        "\n",
        "An example of the GPT-2 architecture:\n",
        "\n",
        "<img src=\"https://github.com/aygul0790/Bootcamp/blob/main/Day_1/pics/GPT2.png?raw=1\" width=\"500\"/>\n",
        "\n",
        "- Huge amounts of data and training\n",
        "- 175B weights\n",
        "- Reinforcement Learning from Human Feedback (RLHF): a technique used to train machine learning models, particularly in natural language processing (NLP), by incorporating human preferences into the training process. It's widely used in fine-tuning large language models (LLMs), such as ChatGPT, to align their responses with human expectations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVY7DAvEkbvi"
      },
      "source": [
        "## 7Ô∏è‚É£Recap and Further Reading\n",
        "\n",
        "- [Intro to Transformers](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro): A high level overview of many of the concepts covered today\n",
        "\n",
        "- [Jay Alamar's Transformer Illustrated](https://jalammar.github.io/illustrated-transformer/): Fantastic step-by-step visualizations and explanations for multiple Transformer-based models\n",
        "\n",
        "- [HuggingFace + Transformers Textbook](https://www.oreilly.com/library/view/natural-language-processing/9781098136789/): If you want to do anything with HuggingFace, this is a fantastic primer.\n",
        "\n",
        "- [Accompanying Open Source GH Repo for the above textbook](https://github.com/nlp-with-transformers/notebooks): Totally free access to the examples from the above book!\n",
        "\n",
        "- [StatsQuest Video on Self-Attention](https://www.youtube.com/watch?v=zxQyTK8quyY): A great step by step breakdown of the process"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bootcamp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}