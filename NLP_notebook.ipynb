{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aygul0790/Bootcamp/blob/main/NLP_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGDiB1hjaCTv"
      },
      "source": [
        "## 1. Text Preprocessing\n",
        "For any Machine Learning algorithm, data preprocessing is crucial, and this remains true for algorithms dealing with text\n",
        "\n",
        "‚úçÔ∏è Text preprocessing is quite different from numerical preprocessing. The most common preprocessing tasks for textual data are:\n",
        "\n",
        "- lowercase\n",
        "- dealing with numbers, punctuation, and symbols\n",
        "- splitting\n",
        "- tokenizing\n",
        "- removing \"stopwords\"\n",
        "- lemmatizing\n",
        "\n",
        "<br>\n",
        "\n",
        "### üíª üßπ Basic cleaning with Python core string operations\n",
        "When you have some unstructured text, you can already clean it with some Python built-in string operations\n",
        "\n",
        "<br>\n",
        "\n",
        "üíª ‚úÇÔ∏è [strip](https://docs.python.org/3/library/stdtypes.html#str.strip) (1/2)\n",
        "\n",
        "`strip` removes all the whitespaces at the beginning and the end of a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Je_zkC5XaCTx",
        "outputId": "c6c2b3a5-e0cb-40ec-e2a4-4e08cba2930c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['   Bonjour, comment ca va ?     ',\n",
              " '    Heyyyyy, how are you doing ?   ',\n",
              " '        Hallo, wie gehts ?     ']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts = [\n",
        "    '   Bonjour, comment ca va ?     ',\n",
        "    '    Heyyyyy, how are you doing ?   ',\n",
        "    '        Hallo, wie gehts ?     '\n",
        "]\n",
        "texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czR1izVJaCTy",
        "outputId": "1f58e2bc-bf44-479e-d325-4d38320e858d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Bonjour, comment ca va ?',\n",
              " 'Heyyyyy, how are you doing ?',\n",
              " 'Hallo, wie gehts ?']"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[text.strip() for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gOThUfXaCTy"
      },
      "source": [
        "üíª ‚úÇÔ∏è strip (2/2)\n",
        "\n",
        "You can also specify a \"list\" of characters (in the form of a single and unordered string) to be removed at the beginning and at the end of a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBGXvWYNaCTz",
        "outputId": "45fed27e-f598-4dbe-ce2f-5cc2de2f71ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"abcd Who is abcd ? That's not a real name!!! abcd\""
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"abcd Who is abcd ? That's not a real name!!! abcd\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJkc7vyWaCTz",
        "outputId": "64e5c444-8403-4d58-80bd-8acf3519779e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" Who is abcd ? That's not a real name!!! \""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text.strip('bdac')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgTVv7wBaCTz"
      },
      "source": [
        "üíª üîÑ [replace](https://docs.python.org/3/library/stdtypes.html#str.replace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-dvNidGaCTz",
        "outputId": "51ecbecb-1edd-40b7-ef8e-d5e546a850e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I love koalas, koalas are the cutest animals on Earth.'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"I love koalas, koalas are the cutest animals on Earth.\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG42XZqQaCT0",
        "outputId": "633760f7-cc97-4831-8837-75b6d2e79cd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I love pandas, pandas are the cutest animals on Earth.'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text.replace(\"koala\", \"panda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOZVkoq-aCT0"
      },
      "source": [
        "üíª üìè [split](https://docs.python.org/3/library/stdtypes.html#str.split)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btDTu1B6aCT0"
      },
      "outputs": [],
      "source": [
        "text = \"linkin park / metallica /red hot chili peppers\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IffxTS-laCT0",
        "outputId": "d77e124a-aa18-4752-836f-58624b1eae4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['linkin park ', ' metallica ', 'red hot chili peppers']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text.split(\"/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8nkW0rMaCT0"
      },
      "source": [
        "üíª üî° Lowercase\n",
        "\n",
        "Text modeling algorithms are case-sensitive (the capitalization of words carries meanings and contexts). Two words need to have the same casing to be considered equal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whXuFAWWaCT0",
        "outputId": "daad80a4-63ef-4a1c-aa4b-bc7c696a4994"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"i LOVE football sO mUch. FOOTBALL is my passion. Who else loves fOOtBaLL ?\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0FZT-a8aCT1",
        "outputId": "d1003f97-d7e2-46f3-e49a-e2c526890b85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i love football so much. football is my passion. who else loves football ?'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re6-di68aCT1"
      },
      "source": [
        "üíª üî¢ Numbers\n",
        "\n",
        "‚úÖ We can (and often should) remove numbers during the text preprocessing steps, especially for:\n",
        "\n",
        "- text clustering\n",
        "- collecting keyphrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uAScWgZaCT1",
        "outputId": "bdbe3dc8-d668-4d2b-9cc5-0b29118aeb5c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"i do not recommend this restaurant, we waited for so long, like 30 minutes, this is ridiculous\"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zksvHSBxaCT1",
        "outputId": "3de790fb-59be-4960-831d-596bf004ca0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'i do not recommend this restaurant, we waited for so long, like  minutes, this is ridiculous'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_text = ''.join(char for char in text if not char.isdigit())\n",
        "cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPnQuDeLaCT1"
      },
      "source": [
        "üíª ‚ùóÔ∏è‚ùìPunctuation and Symbols\n",
        "\n",
        "Punctuation like \".?!\" and symbols like \"@#$\" are not useful for topic modeling.\n",
        "\n",
        "Punctuation is barely used properly on social media platforms.\n",
        "\n",
        "Warning: you might want to keep punctuation and symbols for authorship attribution (Authorship attribution is the task of identifying the author of a given text.)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYMErRWwaCT1",
        "outputId": "cf449939-e956-4b19-d781-8ecd3a876563"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ '"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"I love bubble tea! OMG so #tasty @channel XOXO @$ ^_^ \"\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3yiDed9aCT1",
        "outputId": "93eb08be-617a-4f57-c90f-95bd642569f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string # \"string\" module is already installed with Python\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRJYwSONaCT2",
        "outputId": "92dec157-9e13-4ad5-c9d4-a80b9652cc13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'I love bubble tea OMG so tasty channel XOXO   '"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for punctuation in string.punctuation:\n",
        "    text = text.replace(punctuation, '')\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnENVUhSaCT2"
      },
      "source": [
        "üíª Combination: `strip` + `lowercase` + `numbers` + `punctuation/symbols`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3bjcx5UaCT2"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"   I LOVE Pizza 999 @^_^\",\n",
        "    \"  Study is amazing, take care - 666\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3I8-h51aCT2"
      },
      "outputs": [],
      "source": [
        "def basic_cleaning(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
        "\n",
        "    for punctuation in string.punctuation:\n",
        "        sentence = sentence.replace(punctuation, '')\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0389UCpaCT2",
        "outputId": "772fd037-1f93-4c93-b48f-e8dfacf0a30d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i love pizza', 'study is amazing take care']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned = [basic_cleaning(sentence) for sentence in sentences]\n",
        "cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suz6x43LaCT2"
      },
      "source": [
        "üíª üîç Removing Tags with [RegEx](https://regexr.com/)\n",
        "\n",
        "We can remove HTML tags using RegEx:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjVLQGrAaCT2",
        "outputId": "97272654-b4a9-4ffd-ecf3-8c6f14fd660f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello HSLU!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"<head><body>Hello HSLU!</body></head>\"\"\"\n",
        "cleaned_text = re.sub('<[^<]+?>','', text)\n",
        "\n",
        "print (cleaned_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9z0At9baCT2"
      },
      "source": [
        "We can also extract e-mail addresses from a text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqCnd5aoaCT2",
        "outputId": "59595fcb-e802-4580-b94f-9e55ce7cad7e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:8: SyntaxWarning: invalid escape sequence '\\w'\n",
            "/tmp/ipykernel_83170/3968755570.py:8: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  re.findall('[\\w.+-]+@[\\w-]+\\.[\\w.-]+', txt)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['darkvador@gmail.com', 'batman@outlook.com']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "txt = '''\n",
        "    This is a random text, authored by darkvador@gmail.com\n",
        "    and batman@outlook.com, WOW!\n",
        "'''\n",
        "\n",
        "re.findall('[\\w.+-]+@[\\w-]+\\.[\\w.-]+', txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLYaElzCaCT3"
      },
      "source": [
        "## üíª Cleaning with NLTK\n",
        "\n",
        "Natural Language Toolkit (NLTK) is an NLP library that provides preprocessing and modeling tools for text data\n",
        "\n",
        "üìö [NLTK official website](https://www.nltk.org/)\n",
        "\n",
        "üõ† [Installation Documentation](https://www.nltk.org/install.html)\n",
        "\n",
        "### üíª üß© Tokenizing\n",
        "\n",
        "Tokenizing is essentially splitting a sentence, a paragraph, or even an entire piece of text into smaller chunks such as individual words called tokens.\n",
        "\n",
        "\"Natural Language Processing\"  ‚Üí   [\"Natural\",\"Language\",\"Processing\"]\n",
        "\n",
        "üìö [nltk.tokenize](https://www.nltk.org/api/nltk.tokenize.html)\n",
        "\n",
        "üîÖ Here is a quote from Aristotle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5TUSACvaCT3",
        "outputId": "f263830f-8eec-4376-c5d1-7da633609420"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'It is during our darkest moments that we must focus to see the light'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'It is during our darkest moments that we must focus to see the light'\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db4-CGhjaCT3",
        "outputId": "9600c979-b43b-4974-a6ea-f345a3f87786"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/aygul/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /home/aygul/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /home/aygul/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /home/aygul/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /home/aygul/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# When importing nltk for the first time, we need to also download a few built-in libraries\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JhXDY4IaCT3",
        "outputId": "2dbcaffd-4165-4103-fe6c-e4b1511f6559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['It', 'is', 'during', 'our', 'darkest', 'moments', 'that', 'we', 'must', 'focus', 'to', 'see', 'the', 'light']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "print(word_tokens) # print displays the words in one line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcYjn2iTaCT3"
      },
      "source": [
        "üíª üõë Stopwords\n",
        "\n",
        "Stopwords are words that are used so frequently that they don't carry much information, especially for topic modeling\n",
        "\n",
        "NLTK has a built-in corpus of English stopwords that can be loaded and used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XlVQTtVaCT3"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english')) # you can also choose other languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExEItNKdaCT3"
      },
      "source": [
        "Here is an example of a tokenized sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf4vpb07aCT3"
      },
      "outputs": [],
      "source": [
        "tokens = [\"i\", \"am\", \"going\", \"to\", \"go\", \"to\", \"the\",\n",
        "        \"club\", \"and\", \"party\", \"all\", \"night\", \"long\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTG5eOBraCT3"
      },
      "source": [
        "‚ùì What stopwords could be removed ‚ùì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQ8zzj_YaCT4",
        "outputId": "7f5ec4b9-1a45-42cc-c5fb-afb07b978a7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['i', 'am', 'to', 'to', 'the', 'and', 'all']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopwords_removed = [w for w in tokens if w in stop_words]\n",
        "stopwords_removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj7ZTc2EaCT5"
      },
      "source": [
        "‚ùì What are the meaningful words in this sentence ‚ùì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L41qdw9ZaCT5",
        "outputId": "442067fa-9338-4995-c2ef-d33de158076b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['going', 'go', 'club', 'party', 'night', 'long']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_cleaned = [w for w in tokens if not w in stop_words]\n",
        "tokens_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1rmZ-AbaCT5"
      },
      "source": [
        "üëâ What if you are not going to the party?\n",
        "\n",
        "üò± \"not\" is also considered as a stopword\n",
        "\n",
        "‚úÖ Removing stopwords is useful for:\n",
        "\n",
        "- topic modeling\n",
        "\n",
        "‚ùå Dangerous for:\n",
        "\n",
        "- sentiment analysis\n",
        "- authorship attribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3kXJvJ5aCT5"
      },
      "source": [
        "## üíª üß¨ Lemmatizing\n",
        "Lemmatizing is a technique used to find the root of words, in order to group them by their meaning rather than by their exact form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DarKdtQ-aCT5"
      },
      "source": [
        "![lemmatizing](https://github.com/aygul0790/Bootcamp/blob/main/pics/stem_lemma.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXthx_5PaCT5"
      },
      "source": [
        "üìö [nltk.stem - WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
        "\n",
        "üëá Look at the following sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZtzk-MQaCT5"
      },
      "outputs": [],
      "source": [
        "sentence = 'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uju4Kli6aCT6",
        "outputId": "31673efe-2a19-46dc-a42b-17df9903013b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'He was RUNNING and EATING at the same time =[. He has a bad habit of swimming after playing 3 hours in the Sun =/'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpRKDZa4aCT6"
      },
      "source": [
        "üóì Let's apply the following steps:\n",
        "\n",
        "- Basic cleaning\n",
        "- Tokenizing\n",
        "- Removing stopwords (if not doing sentiment analysis!)\n",
        "- Lemmatizing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te_vW23BaCT6"
      },
      "source": [
        "üßπ Step 1: Basic Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox3AQc8WaCT6",
        "outputId": "ad5656f7-b0f4-47df-a376-e3fafbbdfd21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'he was running and eating at the same time  he has a bad habit of swimming after playing  hours in the sun'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cleaned_sentence = basic_cleaning(sentence)\n",
        "cleaned_sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrjfkA3eaCT6"
      },
      "source": [
        "üíª üß© Step 2 : Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK0oinEaaCT6",
        "outputId": "30ac3cea-5267-4b5d-dcf3-5e8353b0f81e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['he', 'was', 'running', 'and', 'eating', 'at', 'the', 'same', 'time', 'he', 'has', 'a', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'hours', 'in', 'the', 'sun']\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentence = word_tokenize(cleaned_sentence)\n",
        "print(tokenized_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJxf0adjaCT6"
      },
      "source": [
        "üõë Step 3: Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG2P7PWlaCT6",
        "outputId": "09d9be05-d847-4f03-aa7a-7ced78f61743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['running', 'eating', 'time', 'bad', 'habit', 'swimming', 'playing', 'hours', 'sun']\n"
          ]
        }
      ],
      "source": [
        "tokenized_sentence_no_stopwords = [w for w in tokenized_sentence if not w in stop_words]\n",
        "print(tokenized_sentence_no_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfmtFIKvaCT7"
      },
      "source": [
        "üíª üß¨ Step 4: Lemmatizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCE3GEsZaCT7"
      },
      "source": [
        "üìö [WordNetLemmatizer](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
        "\n",
        "[Lemmatization with NLTK](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TT6hMGoaCT7"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Lemmatizing the verbs\n",
        "verb_lemmatized = [\n",
        "    WordNetLemmatizer().lemmatize(word, pos = \"v\") # v --> verbs\n",
        "    for word in tokenized_sentence_no_stopwords\n",
        "]\n",
        "\n",
        "# 2 - Lemmatizing the nouns\n",
        "noun_lemmatized = [\n",
        "    WordNetLemmatizer().lemmatize(word, pos = \"n\") # n --> nouns\n",
        "    for word in verb_lemmatized\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOfW0hDNaCT7"
      },
      "source": [
        "‚úÖ Lemmatizing is useful for:\n",
        "\n",
        "- topic modeling\n",
        "- sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5hdHof5aCT7"
      },
      "source": [
        "## Preprocessing Text - Takeaways\n",
        "\n",
        "First of all, we can perform some pre-cleaning operations on the pieces of text of a corpus using Python built-in functions such as:\n",
        "\n",
        "- ‚úÇÔ∏è strip\n",
        "- üîÑ replace\n",
        "- üìè split\n",
        "- üî° lowercase\n",
        "- üî¢ removing numbers\n",
        "- ‚ùóÔ∏è removing punctuation and symbols\n",
        "\n",
        "Next, we can apply preprocessing techniques to prepare the pieces of text for NLP algorithms\n",
        "\n",
        "- üß© Tokenizing\n",
        "- üõë Removing stopwords\n",
        "- üß¨ Lemmatizing\n",
        "\n",
        "\n",
        "ü§î Now that the text is preprocessed, how can it be analyzed by Machine Learning algorithms?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESmnGAJLaCT7"
      },
      "source": [
        "## 2. Vectorizing\n",
        "\n",
        "ü§ñ Machine Learning algorithms cannot process raw text, as it needs to be converted into numbers first\n",
        "\n",
        "**Vectorizing** = the process of converting raw text into a numerical representation\n",
        "\n",
        "There are multiple vectorizing techniques. Among them, we will present:\n",
        "\n",
        "- `Bag-of-Words`\n",
        "- `Tf_idf`\n",
        "- `N-grams`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osvmhyibaCT7"
      },
      "source": [
        "### 2.1. Bag-of-Words representation\n",
        "\n",
        "**Bag-of-Words representation(BoW)** is one of the most simple and effective ways to represent text for Machine Learning models.\n",
        "\n",
        "When using this representation, we are simply counting how often each word appears in each document of a corpus.\n",
        "\n",
        "The count for each word becomes a feature:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDrzRrqFaCT7"
      },
      "source": [
        "üíª `CountVectorizer`\n",
        "\n",
        "In Scikit-Learn, there is a tool called `CountVectorizer` to generate bag-of-words representations of a set of texts\n",
        "\n",
        "üëâ `CountVectorizer` converts a collection of text documents into a matrix of token counts\n",
        "\n",
        "üìö [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "\n",
        "üëá Look at the following sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rNMPeF8aCT7"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "    'the young dog is running with the cat',\n",
        "    'running is good for your health',\n",
        "    'your cat is young',\n",
        "    'young young young young young cat cat cat'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ1SDzqoaCT8"
      },
      "source": [
        "Let's apply the CountVectorizer to generate a Bag-of-Words representation of these four sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbD4DoJBaCT8",
        "outputId": "fd8669bf-0a79-4bd8-ab06-f815d848c790"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 1, 1, 2, 1, 1, 0],\n",
              "       [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1],\n",
              "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1],\n",
              "       [3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "X = count_vectorizer.fit_transform(texts)\n",
        "X.toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Axqq4KOaCT8"
      },
      "source": [
        "ü§î Can you guess which column represents which word?\n",
        "\n",
        "üî• As soon as the `CountVectorizer` is fitted to the text, you can retrieve all the words seen with `get_feature_names_out()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z75A7bkbaCT8",
        "outputId": "9ca7c4d1-0e7d-488c-c80b-57f9364bc226"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['cat', 'dog', 'for', 'good', 'health', 'is', 'running', 'the',\n",
              "       'with', 'young', 'your'], dtype=object)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "count_vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-69k5xkaCT8",
        "outputId": "a429daa7-7224-4401-e072-df6087976862"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>for</th>\n",
              "      <th>good</th>\n",
              "      <th>health</th>\n",
              "      <th>is</th>\n",
              "      <th>running</th>\n",
              "      <th>the</th>\n",
              "      <th>with</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the young dog is running with the cat</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>running is good for your health</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>your cat is young</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>young young young young young cat cat cat</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           cat  dog  for  good  health  is  \\\n",
              "the young dog is running with the cat        1    1    0     0       0   1   \n",
              "running is good for your health              0    0    1     1       1   1   \n",
              "your cat is young                            1    0    0     0       0   1   \n",
              "young young young young young cat cat cat    3    0    0     0       0   0   \n",
              "\n",
              "                                           running  the  with  young  your  \n",
              "the young dog is running with the cat            1    2     1      1     0  \n",
              "running is good for your health                  1    0     0      0     1  \n",
              "your cat is young                                0    0     0      1     1  \n",
              "young young young young young cat cat cat        0    0     0      5     0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "vectorized_texts = pd.DataFrame(\n",
        "    X.toarray(),\n",
        "    columns = count_vectorizer.get_feature_names_out(),\n",
        "    index = texts\n",
        ")\n",
        "\n",
        "display(vectorized_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftTk8gmHaCT8"
      },
      "source": [
        "Be aware that there are some limitations when it comes to the bag-of-words representation:\n",
        "\n",
        "‚ùå A BoW does NOT take into account the order of the words  ‚Üí   hence the name `\"Bag of Words\"`\n",
        "\n",
        "‚ùå A BoW does NOT take into account a document's length  ‚Üí   `Tf-idf` to the rescue\n",
        "\n",
        "‚ùå A BoW does NOT capture document context  ‚Üí   `N-gram` to the rescue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrB3y-u1aCT8"
      },
      "source": [
        "## 2.2. `Tf-idf` Representation\n",
        "\n",
        "Term Frequency (`tf`) & `CountVectorizer`\n",
        "\n",
        "*Idea: The more often a word appears in a document relative to others, the more likely it is that it will be important to this document*\n",
        "\n",
        "Example: if the word elections appears relatively frequently in a document, it is obvious that this document deals with politics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyeI_rv8aCT8"
      },
      "source": [
        "The frequency of a word $x$ in a document $d$ is called **term frequency**, and is denoted by:\n",
        "\n",
        "$\n",
        "TF_{x,d} = \\dfrac{\\text{Number of times term } x \\text{ appears in document } d}{\\text{Total number of terms in the document}}\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb7eH2EeaCT9"
      },
      "source": [
        "‚ùì In our last example, could we compute $tf_{young.document4}$ ‚ùì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tBHgB_haCT9",
        "outputId": "ec00eee4-fa3f-4c3b-d604-b2793a6c3101"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>for</th>\n",
              "      <th>good</th>\n",
              "      <th>health</th>\n",
              "      <th>is</th>\n",
              "      <th>running</th>\n",
              "      <th>the</th>\n",
              "      <th>with</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the young dog is running with the cat</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>running is good for your health</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>your cat is young</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>young young young young young cat cat cat</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           cat  dog  for  good  health  is  \\\n",
              "the young dog is running with the cat        1    1    0     0       0   1   \n",
              "running is good for your health              0    0    1     1       1   1   \n",
              "your cat is young                            1    0    0     0       0   1   \n",
              "young young young young young cat cat cat    3    0    0     0       0   0   \n",
              "\n",
              "                                           running  the  with  young  your  \n",
              "the young dog is running with the cat            1    2     1      1     0  \n",
              "running is good for your health                  1    0     0      0     1  \n",
              "your cat is young                                0    0     0      1     1  \n",
              "young young young young young cat cat cat        0    0     0      5     0  "
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorized_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zRPz2lLaCT9"
      },
      "source": [
        "$tf_{young, document4} = \\dfrac{5 \\text{ counts of \"young\"}}{8 \\text{ total words}} = 0.625 $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh914kOTaCT9"
      },
      "source": [
        "Document Frequency (`df`)\n",
        "\n",
        "*Idea: If a word appears in many documents of a corpus, however, it shouldn't be that important to understand a particular document.*\n",
        "\n",
        "Example: on eurosport.com/football, the word \"football\" appears in every article, hence why the word football on this website is an unimportant word!\n",
        "\n",
        "The number of documents $d$ in a corpus containing the word $x$ is called document frequency (df), and is denoted by $df_{x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AdJI4XbaCT9"
      },
      "source": [
        "‚ùì In our last example, could we compute $df_{cat}$, $df_{young}$, $df_{the}$ ‚ùì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80D9c0XWaCT-",
        "outputId": "04cd1bfb-19ce-4f03-9843-c986e574cbe3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>for</th>\n",
              "      <th>good</th>\n",
              "      <th>health</th>\n",
              "      <th>is</th>\n",
              "      <th>running</th>\n",
              "      <th>the</th>\n",
              "      <th>with</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Document Frequency</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    cat  dog  for  good  health  is  running  the  with  \\\n",
              "Document Frequency    3    1    1     1       1   3        2    1     1   \n",
              "\n",
              "                    young  your  \n",
              "Document Frequency      3     2  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Compute document frequency (DF)\n",
        "document_frequency = (vectorized_texts > 0).sum(axis=0)\n",
        "\n",
        "# Convert DF into a DataFrame format\n",
        "document_frequency = pd.DataFrame([document_frequency], index=[\"Document Frequency\"])\n",
        "\n",
        "# Display the DataFrame\n",
        "display(document_frequency)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmFSs1qCaCT-"
      },
      "source": [
        "If a word $x$ appears in too many documents of a corpus - i.e. if the document frequency $df_{x}$ is too high - the word $x$ won't help us with topic modeling and should be considered irrelevant.\n",
        "\n",
        "Example: on eurosport.com/football/, the word \"football\" won't help us distinguish two articles, one dealing mainly with strategy and another one talking about referee best practices!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75JoogvVaCT-"
      },
      "source": [
        "What if we considered the **relative document frequency** of a word $x$, which can be computed as:\n",
        "\n",
        "$\n",
        "\\dfrac{df_x}{N}\n",
        "$\n",
        "\n",
        "where:\n",
        "- $df_x$ is the number of documents $d$ containing the word $x$,\n",
        "- $N$ is the total number of documents in a corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZq2GjIbaCT-"
      },
      "source": [
        "For the word \"football\" on Eurosport, we would expect this formula to be close to 1 since the number of docs containing the word \"football\" will probably only be slightly less than the total number of docs (out of 100 maybe only 5 don't have the word \"football\", so we get 95/100)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXhdZ0TpaCT-"
      },
      "source": [
        "Idea: A word $x$ in a corpus of texts will be considered important when its **(relative) document frequency** is **low** ‚áî its inverse document frequency $\\dfrac{N}{df_x}$ is high."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgrctyOPaCT-"
      },
      "source": [
        "Again, if the word \"football\" appears in all the articles it is not very useful for helping us identify between two articles, but if only a few documents contain words like \"concussion\" or \"wellbeing\", (e.g. they appear in 2/100 articles) it will be much more useful in determining the topic of that article (they are probably specifically about player wellfare)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICT-wcj-aCT-"
      },
      "source": [
        "**Tf-idf Formula**\n",
        "\n",
        "üí° Thus the intuition of the `term frequency - inverse document frequency` approach is to give a high weight to any term which appears frequently in a single document, but not in too many documents of the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7recCgPaCT-"
      },
      "source": [
        "The weight of a word $x$ in a document $d$ is given by:\n",
        "\n",
        "$$\n",
        "w_{x,d} = tf_{x,d} \\times \\left[ \\log \\left( \\frac{N + 1}{df_x + 1} \\right) + 1 \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $tf_{x,d}$ = $ \\dfrac{\\text{Number of occurrences of word } x \\text{ in document } d}{\\text{Total number of words in document } d} $\n",
        "\n",
        "- $df_x$ = Number of documents $d$ containing the word $x$\n",
        "- $N$ = Total number of documents in a corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "618NzeTuaCT_"
      },
      "source": [
        "## 2.3. üíª TfidfVectorizer\n",
        "\n",
        "`raw documents`  ‚Üí   `matrix of tf-idf features`\n",
        "\n",
        "üìö [sklearn.feature_extraction.text.TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAJlz5DraCT_",
        "outputId": "9952b864-a2a5-4253-83c6-21352ad0f4b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the young dog is running with the cat',\n",
              " 'running is good for your health',\n",
              " 'your cat is young',\n",
              " 'young young young young young cat cat cat']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGgtlL4-aCT_"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE3yXQqraCT_",
        "outputId": "246f56af-5004-4210-8b0f-ff37a4894dd8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>for</th>\n",
              "      <th>good</th>\n",
              "      <th>health</th>\n",
              "      <th>is</th>\n",
              "      <th>running</th>\n",
              "      <th>the</th>\n",
              "      <th>with</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.227904</td>\n",
              "      <td>0.357056</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.227904</td>\n",
              "      <td>0.281507</td>\n",
              "      <td>0.714112</td>\n",
              "      <td>0.357056</td>\n",
              "      <td>0.227904</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.463709</td>\n",
              "      <td>0.463709</td>\n",
              "      <td>0.463709</td>\n",
              "      <td>0.295980</td>\n",
              "      <td>0.365594</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.365594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.470063</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.470063</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.470063</td>\n",
              "      <td>0.580622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.514496</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.857493</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        cat       dog       for      good    health        is   running  \\\n",
              "0  0.227904  0.357056  0.000000  0.000000  0.000000  0.227904  0.281507   \n",
              "1  0.000000  0.000000  0.463709  0.463709  0.463709  0.295980  0.365594   \n",
              "2  0.470063  0.000000  0.000000  0.000000  0.000000  0.470063  0.000000   \n",
              "3  0.514496  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "        the      with     young      your  \n",
              "0  0.714112  0.357056  0.227904  0.000000  \n",
              "1  0.000000  0.000000  0.000000  0.365594  \n",
              "2  0.000000  0.000000  0.470063  0.580622  \n",
              "3  0.000000  0.000000  0.857493  0.000000  "
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiating the TfidfVectorizer\n",
        "tf_idf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Training it on the texts\n",
        "weighted_words = pd.DataFrame(tf_idf_vectorizer.fit_transform(texts).toarray(),\n",
        "                 columns = tf_idf_vectorizer.get_feature_names_out())\n",
        "\n",
        "weighted_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9XrCgFfaCT_"
      },
      "source": [
        "**Controlling the vocabulary size**:\n",
        "\n",
        "In every language, there are many words used in everyday vocabulary:\n",
        "\n",
        "- üá¨üáß English: ~20,000 words\n",
        "- üá´üá∑ French: ~20,000 words\n",
        "- üá©üá™ German: ~20,000 words\n",
        "\n",
        "In a document, we can't afford to vectorize every word!\n",
        "\n",
        "We can, however, control the number of words to be vectorized (*curse of dimensionality*!):\n",
        "\n",
        "üëâ Scikit-Learn allows us to customize the `CountVectorizer` and `TfidVecdtorizer` with key parameters to control vocabulary size.\n",
        "\n",
        "üíª Key parameters of `TfidfVectorizer` (and `CountVectorizer`)\n",
        "- `max_df/min_df`\n",
        "- `max_features`\n",
        "\n",
        "üíª `max_df` (resp. `min_df`)\n",
        "\n",
        "*When building the vocabulary, `CountVectorizer` and `TfidfVectorizer` will remove terms which have a document frequency strictly higher (resp. lower) than the given threshold. `max_df` and `min_df` help us building corpus-specific stopwords.*\n",
        "\n",
        "Example: when classifying pieces of text into \"basketball\" or \"football\", the word \"ball\" would appear too often and would be useless for this classification, it would be better to filter it out using `max_df`\n",
        "\n",
        "**How to use these parameters in practice?**\n",
        "\n",
        "`max_df` (`min_df`) can be either a float between 0.0 and 1.0 or an integer\n",
        "\n",
        "- `max_df` (`min_df`) = 0.5  ‚áî   \"ignore terms that appear in more (less) than 50% of the documents\"\n",
        "- `max_df` (`min_df`) = 20  ‚áî   \"ignore terms that appear in more (less) than 20 documents\"\n",
        "\n",
        "By default, `max_df` = 1.0  ‚áî  no \"frequent\" word will be removed\n",
        "\n",
        "By default, `min_df` = 0.0  ‚áî   no \"infrequent\" word will be removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A49HaeSuaCT_",
        "outputId": "13e1a965-90d7-4aa2-9416-d39e3829dd64"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>dog</th>\n",
              "      <th>for</th>\n",
              "      <th>good</th>\n",
              "      <th>health</th>\n",
              "      <th>is</th>\n",
              "      <th>running</th>\n",
              "      <th>the</th>\n",
              "      <th>with</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Document Frequency</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    cat  dog  for  good  health  is  running  the  with  \\\n",
              "Document Frequency    3    1    1     1       1   3        2    1     1   \n",
              "\n",
              "                    young  your  \n",
              "Document Frequency      3     2  "
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number of occurences of each word\n",
        "document_frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMsTbYL_aCT_",
        "outputId": "cae7258d-e456-41d7-91ca-6a7f3ce569f4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dog</th>\n",
              "      <th>for</th>\n",
              "      <th>good</th>\n",
              "      <th>health</th>\n",
              "      <th>running</th>\n",
              "      <th>the</th>\n",
              "      <th>with</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the young dog is running with the cat</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>running is good for your health</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>your cat is young</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>young young young young young cat cat cat</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           dog  for  good  health  running  \\\n",
              "the young dog is running with the cat        1    0     0       0        1   \n",
              "running is good for your health              0    1     1       1        1   \n",
              "your cat is young                            0    0     0       0        0   \n",
              "young young young young young cat cat cat    0    0     0       0        0   \n",
              "\n",
              "                                           the  with  your  \n",
              "the young dog is running with the cat        2     1     0  \n",
              "running is good for your health              0     0     1  \n",
              "your cat is young                            0     0     1  \n",
              "young young young young young cat cat cat    0     0     0  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Instantiate the CountVectorizer with max_df = 2\n",
        "count_vectorizer = CountVectorizer(max_df = 2) # removing \"cat\", \"is\", \"young\"\n",
        "\n",
        "# Train it\n",
        "X = count_vectorizer.fit_transform(texts)\n",
        "X = pd.DataFrame(\n",
        "    X.toarray(),\n",
        "    columns = count_vectorizer.get_feature_names_out(),\n",
        "    index = texts\n",
        ")\n",
        "\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27g8ElWhaCUA"
      },
      "source": [
        "üíª max_features\n",
        "\n",
        "By specifying `max_features` = $k$ (k being an integer), the `CountVectorizer` (or the `TfidfVectorizer`) will build a vocabulary that only considers the top $k$ tokens ordered by term frequency across the corpus.\n",
        "\n",
        "**How to use \"max_features\" in practice?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpdpDEiLaCUA",
        "outputId": "61e9794e-f3c8-4d38-e809-8dd2b862d7da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cat</th>\n",
              "      <th>is</th>\n",
              "      <th>young</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the young dog is running with the cat</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>running is good for your health</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>your cat is young</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>young young young young young cat cat cat</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           cat  is  young\n",
              "the young dog is running with the cat        1   1      1\n",
              "running is good for your health              0   1      0\n",
              "your cat is young                            1   1      1\n",
              "young young young young young cat cat cat    3   0      5"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# CountVectorizer with the 3 most frequent words\n",
        "count_vectorizer = CountVectorizer(max_features = 3)\n",
        "\n",
        "X = count_vectorizer.fit_transform(texts)\n",
        "X = pd.DataFrame(\n",
        "    X.toarray(),\n",
        "     columns = count_vectorizer.get_feature_names_out(),\n",
        "     index = texts\n",
        ")\n",
        "\n",
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpXEgpD0aCUA"
      },
      "source": [
        "‚úÖ Advantages of the `Tf-idf` representation:\n",
        "\n",
        "- Using relative frequency rather than count is robust to document length\n",
        "- Takes into account the context of the whole corpus\n",
        "\n",
        "‚ùå Disadvantages of the `Tf-idf` representation:\n",
        "\n",
        "- Like the `BoW`, `Tf-idf` does NOT capture the **within-document context** ‚Üí  `N-gram` helps here\n",
        "- Like the `BoW`, the word order is completely disregarded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO3v6Ys6aCUA"
      },
      "source": [
        "### 2.4. `N-grams`\n",
        "\n",
        "Example: the two following sentences have the exact same representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjzeZXVlaCUA"
      },
      "outputs": [],
      "source": [
        "actors_movie = [\n",
        "    \"I like the movie but NOT the actors\",\n",
        "    \"I like the actors but NOT the movie\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saqME1k8aCUA",
        "outputId": "40e39e64-f81d-474f-abca-7d51381282fc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actors</th>\n",
              "      <th>but</th>\n",
              "      <th>like</th>\n",
              "      <th>movie</th>\n",
              "      <th>not</th>\n",
              "      <th>the</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>I like the movie but NOT the actors</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I like the actors but NOT the movie</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     actors  but  like  movie  not  the\n",
              "I like the movie but NOT the actors       1    1     1      1    1    2\n",
              "I like the actors but NOT the movie       1    1     1      1    1    2"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vectorize the sentences\n",
        "count_vectorizer = CountVectorizer()\n",
        "actors_movie_vectorized = count_vectorizer.fit_transform(actors_movie)\n",
        "\n",
        "# Show the representations in a nice DataFrame\n",
        "actors_movie_vectorized = pd.DataFrame(\n",
        "    actors_movie_vectorized.toarray(),\n",
        "    columns = count_vectorizer.get_feature_names_out(),\n",
        "    index = actors_movie\n",
        ")\n",
        "\n",
        "# Show the vectorized movies\n",
        "actors_movie_vectorized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiTeR5v4aCUA"
      },
      "source": [
        "When using a `bag-of-words` representation, an efficient way to capture context is to consider:\n",
        "\n",
        "- the count of single tokens (unigrams)\n",
        "- the count of pairs (bigrams), triplets (trigrams), and more generally sequences of $n$ words, also known as `n-grams`\n",
        "\n",
        "Examples:\n",
        "\n",
        "- \"mathematics\" is a unigram (n = 1)\n",
        "- \"machine learning\" is a bigram (n = 2)\n",
        "- \"natural language processing\" is a trigram (n = 3)\n",
        "- \"deep convolutional neural networks\" is a 4-gram (n = 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp4TDYXEaCUA"
      },
      "source": [
        "üíª `ngram_range`\n",
        "\n",
        "In both `CountVectorizer` and `TfidfVectorizer`, you can specify the length of your sequences with the parameter `ngram_range` = (`min_n`, `max_n`).\n",
        "\n",
        "Examples:\n",
        "\n",
        "- ngram_range = (1, 1) üëâ (by default) will only capture the unigrams (single words)\n",
        "- ngram_range = (1, 2) üëâ will capture the unigrams, and the bigrams\n",
        "- ngram_range = (1, 3) üëâ will capture the unigrams, the bigrams, and the trigrams\n",
        "- ngram_range = (2, 3) üëâ will capture the bigrams, and the trigrams but not the unigrams\n",
        "\n",
        "With a unigram vectorization, we couldn't distinguish two sentences with the same words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDd3UhlBaCUB",
        "outputId": "5da82c31-9dc0-4996-8f20-f9d4ce491c98"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actors</th>\n",
              "      <th>but</th>\n",
              "      <th>like</th>\n",
              "      <th>movie</th>\n",
              "      <th>not</th>\n",
              "      <th>the</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>I like the movie but NOT the actors</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I like the actors but NOT the movie</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     actors  but  like  movie  not  the\n",
              "I like the movie but NOT the actors       1    1     1      1    1    2\n",
              "I like the actors but NOT the movie       1    1     1      1    1    2"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actors_movie_vectorized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h29UHnhmaCUB"
      },
      "source": [
        " What about a **bigram vectorization**?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UxQUG4waCUB",
        "outputId": "6f902892-f639-4a5f-98e5-d971fb7341b6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actors but</th>\n",
              "      <th>but not</th>\n",
              "      <th>like the</th>\n",
              "      <th>movie but</th>\n",
              "      <th>not the</th>\n",
              "      <th>the actors</th>\n",
              "      <th>the movie</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>I like the movie but NOT the actors</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>I like the actors but NOT the movie</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     actors but  but not  like the  movie but  \\\n",
              "I like the movie but NOT the actors           0        1         1          1   \n",
              "I like the actors but NOT the movie           1        1         1          0   \n",
              "\n",
              "                                     not the  the actors  the movie  \n",
              "I like the movie but NOT the actors        1           1          1  \n",
              "I like the actors but NOT the movie        1           1          1  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vectorize the sentences\n",
        "count_vectorizer_n_gram = CountVectorizer(ngram_range = (2,2)) # BI-GRAMS\n",
        "actors_movie_vectorized_n_gram = count_vectorizer_n_gram.fit_transform(actors_movie)\n",
        "\n",
        "# Show the representations in a nice DataFrame\n",
        "actors_movie_vectorized_n_gram = pd.DataFrame(\n",
        "    actors_movie_vectorized_n_gram.toarray(),\n",
        "    columns = count_vectorizer_n_gram.get_feature_names_out(),\n",
        "    index = actors_movie\n",
        ")\n",
        "\n",
        "# Show the vectorized movies with bigrams\n",
        "actors_movie_vectorized_n_gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O_EG6F9aCUB"
      },
      "source": [
        "üëç The two sentences are now distinguishable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXNh8Wx8aCUB"
      },
      "source": [
        "#### **Vectorizing - Takeaways**\n",
        "\n",
        "There are two methods for vectorizing:\n",
        "- `CountVectorizer` (counting)\n",
        "- `TfidfVectorizer` (weighing: take the document length into consideration)\n",
        "\n",
        "The most important parameters of these vectorizers are:\n",
        "- `min_df` (infrequent words)\n",
        "- `max_df` (frequent words)\n",
        "- `max_features` (curse of dimensionality)\n",
        "- `ngram_range` = (`min_n`, `max_n`) (capturing the context of the words)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bootcamp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}